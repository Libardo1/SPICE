\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}


%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Sequence Prediction using graphical Models }

\author{
Akshay Jain\\
Department of Computer Science\\
University of California,Irvine \\
\texttt{akshaj1@uci.edu} \\
\And
Varun Bharill \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
\And
Sai Teja \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Being able to guess the next element of a sequence is an important question in many fields, from natural language processing (NLP) to biology (DNA modelisation for instance). Different algorithms have been suggested in literature to handle the problem. In this paper, we explore Graphical model based algorithms and how they perform with respect to each other.
\end{abstract}

\section{Introduction}
\label{Introduction}
Sequenced data arises very frequently is real world. It appears DNA and RNA sequences, in sentence of words. Sequence can also be generated in a time series fashion, such as Stock prices, rain fall measurement. 

\section{Algorithms}
\label{Algorithms}
Other the period, various algorithms have been developed to predict the future values in the sequence data. 
\subsection{N-Gram Model}

\subsection{Hidden Markov Model(HMMs)}

\subsection{Other}

In this paper, we will focus mainly on N-gram models and Hidden Markov Models.

\section{N-gram Model}
\label{N-Gram Model}

\section{Hidden Markov Model}
\label{HMM}

In simple Markov Model,the states are visible. In Hidden Markov Model, the states are not visible, but the output dependent on the states are observed. Each states has a probability distribution over the possible observations. The sequence of the observations allows to infer the sequence of the hidden states. 

Consider \(x_t\) be the observed data variable, where t denotes the discrete time or position stamp in the sequence 1...T

The complete observed sequence consists of \(D ={x_1,,,,x_T}\). The hidden state \(z_k\) is a discrete random variable with taking one of the possible K values of \(z_k \in \{1...K\}\)

There are two assumptions in Hidden Markov Model:
\begin{enumerate}
\item Observations $x_t$ are conditionlly independent of all other states, given the \(z_t\), i.e the observtion at step \(t\) depends only on the state at time \(t\)
\item The state of the model at time t+1, \(z_{t+1}\) depends only on \(z_t\), i.e 
\( p\big(z_{t+1}|z_t,..z_1\big) = p\big(z_{t+1}|z_t\big) \)
\end{enumerate}

There are three parameters in the model:
\begin{enumerate}
\item The initial distribution of the states is defined as \(p\big(z_1|z_0 \big) = \pi\)
\item Transition matrix A, \(K \times K \), with \(a_{ij} = p\big(z_t=j|z_(t-1)=i\big)\) , \(1\leq i \leq K \).
\item Emission density distribution B, where \(b_{ij} = p\big(x_t=j|z_t=i\big)\).
\end{enumerate}

We will denote \(x_1,...,x_T\) as \(x_{[1,T]}\), and \(z_1,...,z_T\) as \(z_{[1,T]}\). For the observed data \(D ={x_1,,,,x_T}\), the graphical model can be defined as:

\begin{center} \(  p\big( x_{[1,T]}, z_{[1,T]} \big) = \prod_{t=1}^{T}p\big(x_t|z_t\big)p\big(z_t|z_{t-1}\big)  \)
\end{center}

In most of the cases, we do not know the Transition Matrix and the Emission density distribution. Only
the observed data is know. In order to learn the model parameters, we have discussed two algorithms,
Baum-Welch algorithm which uses Expectation Maximization and Spectral Learning, which used SVD.

\section{EM algorithm for HMM}
\label{EM algorithm}
The Baum-Welch algorithm uses the well-known EM algorithm to learn the unknown parameters of the model. We will need to compute the likelihood of the model in order to learn the parameters.

Let $\theta$ denote the model parameters, i.e \( \theta = \big(A,B,\pi\big) \)
Then likelihood \(L\big(\theta\big)\) of the model can be written as 
\[L\big(\theta \big)\ = p\big(x_{[1,T]}|\theta \big) \]
\[L\big(\theta \big)\ = \sum p\big(x_{[1,T]}|\theta \big) \]



\subsection{Efficient Computation of the Likelihood}
The likelihood \(L(\)

\section{Spectral Learning}
\label{Spectral Learning}

\section{Dataset}

\section{Reults}

\begin{table}[t]
\caption{N-Gram Model}
\label{N-Gram_model}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-gram} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{Hidden Markov Model using EM}
\label{HMM_EM}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Hidden Markov Model using Spectral Learning}
\label{Spectral Learning}
\begin{center}
\begin{tabular}{ |l|l|l|l| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-gram} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
