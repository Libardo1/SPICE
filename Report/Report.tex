\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}


%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Sequence Prediction using graphical Models }

\author{
Akshay Jain\\
Department of Computer Science\\
University of California,Irvine \\
\texttt{akshaj1@uci.edu} \\
\And
Varun Bharill \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
\And
Sai Teja Ranuva \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Being able to guess the next element of a sequence is an important question in many fields, from natural language processing (NLP) to biology (DNA modelisation for instance). Different algorithms have been suggested in literature to handle the problem. In this paper, we explore Graphical model based algorithms and how they perform with respect to each other.
\end{abstract}

\section{Introduction}
\label{Introduction}
Sequence prediction is an important task in machine learning. Sequence prediction can be defined as the task of predicting the next element given the previous element in a sequence. This task appears naturally in stock market prediction, user browsing action prediction, word prediction in natural languages. Sequence prediction problem also arises prominently in time series data rain fall measurement etc.

There are various machine learning techniques developed or existing techniques applied to effectively predict the next element in a sequence. The algorithms for sequence prediction can be broadly classified into graphical models, models based on expert advice and recurrent neural network models. Recurrent neural networks and graphical models have been investigated extensively. 

Word prediction models can help correct misspelled words in a sentence. Prediction in natural languages do not have to deal with long sequences and also next element in the sentence may depend only words last three or four words. This makes n-gram models very effective in natural language processing and some would argue that with rich n-gram data like Google n-gram data set one need not look outside n-gram models. Word prediction models are being used extensively by various chat messengers like Whatsapp, messenger to aid in texting by predicting the next word that might be typed.

In case of gene sequence prediction, DNA modelisation, the sequences are long and the next element in the sequence might depend on the the entire sequence seen till now. This calls for much more complex models to predict effectively. Hidden Markov Models have been found to be effective in such scenarios. 

The study of sequence prediction has always been tailored specific to a particular field like Natural Language processing, bioinformatics(RNA, DNA sequences) etc which is actually effective. In SPICE challenge, the objective is to predict well on the data without knowing the source of it. The way to go about it would be to compare different approaches on the same data and pick the best performing model. In particular we wish to study N-gram models, Hidden Markov Model using Expectation Maximization and Spectral method and compare the results.

\section{N-gram Model}
\label{N-Gram Model}

\section{Hidden Markov Model}
\label{HMM}

A Hidden Markov Model is a Markov Model, in which the states being modeled are assumed to be hidden. In Markov Model, the states are directly visible. In Hidden Markov Model, the states are not visible, but the output, dependent on the states are observed. Each states has a probability distribution over the possible observations. The sequence of the observations allows to infer the sequence of the hidden states. 

Consider \(y_t\) be the observed data variable, where t denotes the discrete time or position stamp in the sequence 1...T

The complete observed sequence consists of \(D ={y_1,,,,y_T}\). The observation variable $y_t$ can take one of the N values of \(y_n \in \{1...n\}\). The hidden state \(x_k\) is a discrete random variable with taking one of the possible K values of \(x_k \in \{1...K\}\)

There are two assumptions in Hidden Markov Model:
\begin{enumerate}
\item Observations $y_t$ are conditionally independent of all other states, given the \(x_t\), i.e the observation at step \(t\) depends only on the state at time \(t\)
\item The state of the model at time t+1, \(x_{t+1}\) depends only on \(x_t\), i.e 
\( p\big(x_{t+1}|x_t,..x_1\big) = p\big(x_{t+1}|x_t\big) \)
\end{enumerate}

There are three parameters in the model:
\begin{enumerate}
\item The initial distribution of the states is defined as \(p\big(x_1|x_0 \big) = \pi\)
\item Transition matrix A, \(K \times K \), with \(a_{ij} = p\big(x_t=j|x_(t-1)=i\big)\) , \(1\leq i \leq K \).
\item Emission density distribution \(B = \{b\big( b\big)\}\) , where \(b_{j} = p\big(y_t=j|x_t=i\big)\).
\end{enumerate}

We will denote \(y_1,...,y_T\) as \(y_{[1,T]}\), and \(x_1,...,x_T\) as \(x_{[1,T]}\). For the observed data \(D ={y_1,,,,y_T}\), the graphical model can be defined as:

\begin{center} \(  p\big( y_{[1,T]}, y_{[1,T]} \big) = \prod_{t=1}^{T}p\big(y_t|x_t\big)p\big(x_t|x_{t-1}\big)  \)
\end{center}

In most of the cases, we do not know the Transition Matrix and the Emission density distribution. Only
the observed data is known. In order to learn the model parameters, we have discussed two algorithms in the following sections. First one is the Baum-Welch algorithm which uses Expectation Maximization and the second one is Spectral Learning, which uses SVD.

\section{EM algorithm for HMM}
\label{EM algorithm}
The Baum-Welch algorithm uses the well-known EM algorithm to learn the unknown parameters of the model. It makes use of the forward-backward pass algorithm. 

\subsection*{}{Algorithm}
Let $\theta$ denote the model parameters, i.e \( \theta = \big(A,B,\pi\big) \)

\subsection*{Forward Pass}
Let $\alpha_i\big(t\big) = P\big(x_t=i, y_{[1,t]}\big)$, the probability of observing  $y_{[1,t]}$, being in state $i$ at time $t$. $\alpha_i\big(t\big)$ can be computed recursively using 



\subsection*{Backward Pass}

\subsection*{Update}

Then likelihood \(L\big(\theta\big)\) of the model can be written as 
\[L\big(\theta \big)\ = p\big(x_{[1,T]}|\theta \big) \]
\[L\big(\theta \big)\ = \sum_{z_{[1,T]}} p\big(x_{[1,T]},z_{[1,T]}|\theta \big) \]



\section{Spectral Learning}
\label{Spectral Learning}

In the above section we have discussed the EM algorithm for learning the parameters (transition matrix, emission densities and initial distribution ) of a Hidden Markov Model. One major drawback of the above algorithm is that it can possibly converge to a local optima. Since learning an HMM can be hard (Terwijin 2002 ) in some scenarios, several heuristics based approaches have been suggested for the learning process. However, in practical applications, the hardness scenarios are typically less encountered and under certain assumption a more accurate spectral learning algorithm has been proposed by Daniel Hsu , et. al.  This algorithm enables us to efficiently perform the following tasks.
\begin{enumerate}
	\item Approximate the joint liklihood of an observed sequence of length \textit{t}. The quality of approximation degrades as \textit{t} increases.
	\item Given a future observation, approximate its conditional distribution given some previous observation. 
\end{enumerate}
In the analysis presented in this report we are performing both of the above task to approximate the distribution of a future symbol.
\subsection{Priliminaries}
In this section we will we will briefly describe how an HMM can be represented in a matrix form, followed by the basic notation used in explaining the spectral algorithm. 
\newline
We will first consider the matrix notation of forward algorithm (as described in previous sections) an HMM. For each symbol /x, define a matrix $A_x$ as follows -
\begin{center}
$[A_x]_{h',h}$ = \textit{t}(h'$\vert$h)o(x$\vert$h) 
\end{center}
 
where t(h'$\vert$h) is the transition matrix and o is the emission density matrix.
Now consider a sequence of observed symbols as, $o_1,o_2...o_T$. The joint liklihood of the distribution given by $p(o_1....o_T)$ can be represented as -
\begin{center}
$p(o_1....o_T) = 1^T \times A_{o_T} \times A_{o_{T-1}} . . . . \times A_{o_1} \times \pi$
\end{center}
where $\pi$ is the initial state distribution. We will see that the spectral learning algorithm described in the later sections approximates $A_x$ and there by computes the joint distribution.

\subsection{Notation}
Before we describe the spectral learning algorithm, consider the following notations.
\begin{enumerate}
\item For each symbol \textit{x} define, 
\begin{center}
$[P_{3,x,1}]_{i,j} = P(X_3 = i, X_2 = x, X_1 = j)$
\end{center}
where $[P_{3,x,1}]_{i,j}$ denotes the joint probability distribution of the neighboring symbols for a current symbol.
\item For every symbol \textit{x} define,
\begin{center}
$[P_{2,1}]_{i,j} = P(X_2 = j, X_1 = i)$
\end{center}
where $[P_{2,1}]_{i,j}$ denotes the joint probability distribution for consecutive symbols in the observation sequence.
\end{enumerate}
\subsection{Spectral learning Algorithm}
Given the list of sequences of observed symbols, following steps outline the spectral learning algorithm.
\newline
\newline
\textbf{Step 1.} Estimate the empirical approximations of $[P_{3,x,1}]_{i,j}$ and $[P_{2,1}]_{i,j}$ as follows - 
\begin{enumerate}
 \item $[\hat{P}_{2,1}]_{i,j} = \frac{Count(X_2 = i, X_1 = j)}{N_{2,1}}$ ,where $N_{2,1}$ is the number total consecutive pairs. and,
\item $[\hat{P}_{3,x,1}]_{i,j} = \frac{Count(X_3 = i, X_2 = x, X_1 = j)}{N_{3,x,1}}$ ,where $N_{3,x,1}$ is the total number of triples for a particular symbol x.
\end{enumerate}

\textbf{Step 2.} Compute singular value decomposition of $[\hat{P}_{2,1}]_{i,j}$ 
\begin{center} SVD($[\hat{P}_{2,1}]_{i,j}$) = $U \in R^{n\times m} ,  \Sigma \in R^{m\times m} , V \in R^{n \times m}$
\end{center}
where \textit{m} denotes the rank and is analogous to the number of hidden states in our algorithm.

\textbf{Step 3.} For each symbol x compute $B_x$ as follows -
\begin{center}
$B_x = U^T \times [P_{3,x,1}] \times V \times \Sigma^{-1}$
\end{center}

At this point, it is important that we state the following theorem. We skip the proof for brevity.

\textbf{Theorem 1.} If $P_{2,1}$ is of rank m then,
\begin{center}
$B_x = G \times A_X \times G^{-1}$
\end{center}
where $G \in R^{m \times m}$ is an invertible matrix.

\textbf{Step 4.} Compute $B_{0}$ and $B_{\inf}$
\begin{enumerate}
\item $b_{0} = G\pi$ 
\item $b_{\inf} = 1^TG^{-1}$
\end{enumerate}

\textbf{Step 5.} For a given sequence $o_1$, $o_2$, ... $o_T$ and a future symbol $o_{T+1}$ we can compute the joint liklihood of the sequence and the conditional distribution of the future symbol as follows.
\begin{enumerate}
\item $\hat{p}(o_1, o_2, ... o_T) = b^{\inf} \times B_{o_{T}} . . . B_{o_{1}} \times b^{0}$, 
\newline where $\hat{p}(o_1, o_2, ... o_T)$ represents the joint liklihood. Note that all the G's cancel out and we arrive at the forward matrix form of an hmm.

\item $\hat{p}(o_{T+1} \Vert o_{1}...o_{T} ) = \frac{\hat{p}(o_1, o_2, ... o_T, o_{T+1})}{\hat{p}(o_1, o_2, ... o_T)} \propto \hat{p}(o_1, o_2, ... o_T, o_{T+1})$ 

\end{enumerate}


\section{Dataset}
\section{Reults}

\begin{table}[t]
\caption{N-Gram Model}
\label{N-Gram_model}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-gram} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{Hidden Markov Model using EM}
\label{HMM_EM}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Hidden Markov Model using Spectral Learning}
\label{Spectral Learning}
\begin{center}
\begin{tabular}{ |l|l|l|l| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf \# Observation symbols} &\multicolumn{1}{|c|}{\bf \# hidden states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 2}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 2}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 3}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 4}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
