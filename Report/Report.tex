\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Sequence Prediction using graphical Models }

\author{
Akshay Jain\\
Department of Computer Science\\
University of California,Irvine \\
\texttt{akshaj1@uci.edu} \\
\And
Varun Bharill \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
\And
Sai Teja \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Being able to guess the next element of a sequence is an important question in many fields, from natural language processing (NLP) to biology (DNA modelisation for instance). Different algorithms have been suggested in literature to handle the problem. In this paper, we explore Graphical model based algorithms and how they perform with respect to each other.
\end{abstract}

\section{Introduction}
\label{Introduction}
Sequenced data arises very frequently is real world. It appears DNA and RNA sequences, in sentence of words. Sequence can also be generated in a time series fashion, such as Stock prices, rain fall measurement. 

\section{Algorithms}
\label{Algorithms}
Other the period, various algorithms have been developed to predict the future values in the sequence data. 
\subsection{N-Gram Model}

\subsection{Hidden Markov Model(HMMs)}

\subsection{Other}

In this paper, we will focus mainly on N-gram models and Hidden Markov Models.

\section{N-gram Model}
\label{N-Gram Model}

\section{Hidden Markov Model}
\label{HMM}

In simple Markov Model,the states are visible. In Hidden Markov Model, the states are not visible, but the output dependent on the states are observed. Each states has a probability distribution over the possible observations. The sequence of the observations allows to infer the sequence of the hidden states. 

Consider \(x_t\) be the observed data variable, where t denotes the discrete time or position stamp in the sequence 1...T

The complete observed sequence consists of \(D ={x_1,,,,x_T}\)

\subsection{Learning Model Parameters}
\subsubsection{EM algorithm}
\subsubsection{Spectral Learning}
In the above section we have discussed the EM algorithm for learning the parameters (transition matrix, emission densities and initial distribution ) of a Hidden Markov Model. One major drawback of the above algorithm is that it can possibly converge to a local optima. Since learning an HMM can be hard (Terwijin 2002 ) in some scenarios, several heuristics based approaches have been suggested for the learning process. However, in practical applications, the hardness scenarios are typically less encountered and under certain assumption a more accurate spectral learning algorithm has been proposed by Daniel Hsu , et. al.  This algorithm enables us to efficiently perform the following tasks.
\begin{enumerate}
	\item Approximate the joint liklihood of an observed sequence of length \textit{t}. The quality of approximation degrades as \textit{t} increases.
	\item Given a future observation, approximate its conditional distribution given some previous observation. 
\end{enumerate}
In the analysis presented in this report we are performing both of the above task to approximate the distribution of a future symbol.
\subsubsection{Priliminaries}
In this section we will we will briefly describe how an HMM can be represented in a matrix form, followed by the basic notation used in explaining the spectral algorithm. 
\newline
We will first consider the matrix notation of forward algorithm (as described in previous sections) an HMM. For each symbol /x, define a matrix $A_x$ as follows -
\begin{center}
$[A_x]_{h',h}$
\end{center}


\section{EM algorithm}
\label{EM algorithm}
\section{Spectral Learning}
\label{Spectral Learning}

\section{Dataset}

\section{Reults}

\begin{table}[t]
\caption{N-Gram Model}
\label{N-Gram_model}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-gram} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{Hidden Markov Model using EM}
\label{HMM_EM}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Hidden Markov Model using Spectral Learning}
\label{Spectral Learning}
\begin{center}
\begin{tabular}{ |l|l|l|l| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf \# Observation symbols} &\multicolumn{1}{|c|}{\bf \# hidden states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 2}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 2}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 3}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 4}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
