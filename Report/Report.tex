\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{newunicodechar}
\usepackage{float}

% The offending character in the first argument
% and a hyphen in the second argument!
\newunicodechar{?}{-}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Sequence Prediction using graphical Models }

\author{
Akshay Jain\\
Department of Computer Science\\
University of California,Irvine \\
\texttt{akshaj1@uci.edu} \\
\And
Varun Bharill \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{vbharill@uci.edu} \\
\And
Sai Teja Ranuva \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{ranuva.teja@gmail.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Being able to guess the next element of a sequence is an important question in many fields, from natural language processing (NLP) to biology (DNA modelisation for instance). Different algorithms have been suggested in literature to handle the problem. In this paper, we explore Graphical model based algorithms and how they perform with respect to each other.
\end{abstract}

\section{Introduction}
\label{Introduction}
Sequence prediction is an important task in machine learning. Sequence prediction can be defined as the task of predicting the next element given the previous element in a sequence. This task appears naturally in stock market prediction, user browsing action prediction, word prediction in natural languages. Sequence prediction problem also arises prominently in time series data rain fall measurement etc.

There are various machine learning techniques developed or existing techniques applied to effectively predict the next element in a sequence. The algorithms for sequence prediction can be broadly classified into graphical models, models based on expert advice and recurrent neural network models. Recurrent neural networks and graphical models have been investigated extensively. 

Word prediction models can help correct misspelled words in a sentence. Prediction in natural languages do not have to deal with long sequences and also next element in the sentence may depend only words last three or four words. This makes n-gram models very effective in natural language processing and some would argue that with rich n-gram data like Google n-gram data set one need not look outside n-gram models. Word prediction models are being used extensively by various chat messengers like Whatsapp, messenger to aid in texting by predicting the next word that might be typed.

In case of gene sequence prediction, DNA modelisation, the sequences are long and the next element in the sequence might depend on the the entire sequence seen till now. This calls for much more complex models to predict effectively. Hidden Markov Models have been found to be effective in such scenarios. 

The study of sequence prediction has always been tailored specific to a particular field like Natural Language processing, bioinformatics(RNA, DNA sequences) etc which is actually effective. In SPICE challenge, the objective is to predict well on the data without knowing the source of it. The way to go about it would be to compare different approaches on the same data and pick the best performing model. In particular we wish to study N-gram models, Hidden Markov Model using Expectation Maximization and Spectral method and compare the results.

\section{N-gram Model}
\label{N-Gram Model}

An N-gram is a continuous sequence of N elements in a sequence. The intuition of N-gram models is that we can approximate the history of the sequence seen by the last N elements of the sequence. m model, for example, approximates the probability of an element given all the previous elements $P\big(s_n|s_{n-1}\big)$ by using only the conditional probability of the preceding elements $P\big(s_n|s_{n?1}\big)$. This assumption that the probability of an element depends only on the previous element is called a Markov assumption. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. We can generalize the bigram to the trigram and then to N-grams. N-gram model is also called as $\big(N-1\big)^{th}$ order markov model.

Thus, the general equation for this N-gram or $\big(N-1\big)^{th}$ order markov model approximation to the conditional probability of the next element in a sequence is as follows:

Let $s$ be a sequence of elements

$p\big(s_n|s_1, s_2, . . . s_{n-1}\big) = p\big(s_n|s_{n-N+1}, . . . , s_{n-1}\big)$

\subsection{Estimating N-gram Probabilities}
We can estimate the N-gram probabilities using Maximum Likelihood Estimates 

%$p\big(s_n|s_{n-N+1}, . . . , s_{n-1}\big) = \frac{Count\big(s_n,s_{n-N+1}, . . . , s_{n-1}\big)}$

Thus, the general equation for this N-gram approximation to the conditional probability of the next word in a sequence is as follows:

$p\big(s_{n}|s_1, . . . , s_{n-1}\big) = \frac{Count\big(s_1, . . . , s_{n}\big)}{Count\big(s_1, . . . , s_{n-1}\big)}$

The above formula gives us a decent approximation of the $p\big(s_{n}|s_1, . . . , s_{nâˆ’1}\big)$. But what if we haven't seen the sequence $s_1, . . . , s_{n}$ in our training data set ? We might have to assign a zero probability to the sequence. But that might not be a right thing to do. The technique used in machine learning to deal with unseen data is to pull some probability from other sequences and add it to these unseen sequences. This is done using Add 1 smoothing. This process is called smoothing. In Add one smoothing, while calculating the probabilities one has to add one to the numerator and V to the denominator i.e as follows.\\
$p\big(s_{n}|s_1, . . . , s_{n-1}\big) = \frac{1 + Count\big(s_1, . . . , s_{n}\big)}{V + Count\big(s_1, . . . , s_{n-1}\big)}$

where V is the total number of possible (N-1)-grams 

This method works most of the times in various problem settings. But it hits a problem if we are dealing with too many unseen N-grams. It then sucks away a lot of probability from the observes N-grams to compensate for the unseen N-grams. This can sometime make the model useless. 

There is another way of dealing with unseen N-grams. This technique is called N-gram backoff technique. In this technique, we first try to find the $p\big(s_{n}|s_1, . . . , s_{n-1}\big)$ by using N-grams, in case we do not observe the given N-gram, we backoff to (N-1)-gram and so on. Lets try to generalize this idea. Let us take an example of 3-gram to illustrate the idea.

We calculate the $p\big(s_{n}|s_{n-2},s_{n-1}\big)$, $p\big(s_{n}|s_{n-1}\big)$ and $p\big(s_{n}\big)$ using 3-gram counts, 2-gram counts and 1-gram counts and MLE estimates as described above. Now we take a linear combination of these three as follows:\\

$p\big(s_{n}|s_1, . . . , s_{n-1}\big) = \lambda_1p\big(s_{n}|s_{n-2},s_{n-1}\big) + \lambda_2p\big(s_{n}|s_{n-1}\big) + \lambda_3p\big(s_{n}\big)$ 

where $\lambda_1 + \lambda_2 + \lambda_3 = 1$

The values of lambdas are usually set using heuristics. We set these values as suggested for NLP as $(\lambda_1, \lambda_2, \lambda_3) = (0.5, 0.3, 0.2)$ for 3-grams.
We shall we investigating this n-gram backoff models on the SPICE challenge data set.


\section{Hidden Markov Model}
\label{HMM}

A Hidden Markov Model is a Markov Model, in which the states being modeled are assumed to be hidden. In Markov Model, the states are directly visible. In Hidden Markov Model, the states are not visible, but the output, dependent on the states are observed. Each states has a probability distribution over the possible observations. The sequence of the observations allows to infer the sequence of the hidden states. 

Consider \(Y_t\) be the observed data variable, where t denotes the discrete time or position stamp in the sequence 1...T

The complete observed sequence consists of \(D ={Y_1,,,,Y_T}\). The observation variable $Y_t$ can take one of the N values of \(Y_n \in \{1...n\}\). The hidden state \(X_k\) is a discrete random variable with taking one of the possible K values of \(X_k \in \{1...K\}\)

There are two assumptions in Hidden Markov Model:
\begin{enumerate}
\item Observations $Y_t$ are conditionally independent of all other states, given the \(X_t\), i.e the observation at step \(t\) depends only on the state at time \(t\)
\item The state of the model at time t+1, \(X_{t+1}\) depends only on \(X_t\), i.e 
\( p\big(X_{t+1}|X_t,..X_1\big) = p\big(X_{t+1}|X_t\big) \)
\end{enumerate}

There are three parameters in the model:
\begin{enumerate}
\item The initial distribution of the states is defined as \(p\big(X_1|X_0 \big) = \pi\)
\item Transition matrix A, \(K \times K \), with \(a_{ij} = p\big(X_t=j|X_(t-1)=i\big)\) , \(1\leq i \leq K \).
\item Emission density distribution \(B = \{b_{j}\big(y_t\big)\}\) , where \(b_{j}\big(y_t\big) = p\big(Y_t=y_t|X_t=j\big)\).
\end{enumerate}

We will denote \(Y_1,...,Y_T\) as \(Y_{[1,T]}\), and \(X_1,...,X_T\) as \(X_{[1,T]}\). For the observed data \(D ={y_1,,,,y_T}\), the graphical model can be defined as:

\begin{center} \(  p\big( Y_{[1,T]}, X_{[1,T]} \big) = \prod_{t=1}^{T}p\big(Y_t|X_t\big)p\big(X_t|X_{t-1}\big)  \)
\end{center}

In most of the cases, we do not know the Transition Matrix and the Emission density distribution. Only
the observed data is known. In order to learn the model parameters, we have discussed two algorithms in the following sections. First one is the Baum-Welch algorithm which uses Expectation Maximization and the second one is Spectral Learning, which uses SVD.

\section{EM algorithm for HMM}
\label{EM algorithm}
The Baum-Welch algorithm uses the well-known EM algorithm to learn the unknown parameters of the model. It makes use of the forward-backward pass algorithm. 

\subsection*{Algorithm}
Let $\theta$ denote the model parameters, i.e \( \theta = \big(A,B,\pi\big) \). It can be initialize randomly, or based on some prior knowledge.

\subsection*{Forward Pass}
Let $\alpha_i\big(t\big) = P\big(X_t=i,Y_{[1,t]}|\theta\big)$, the probability of observing  $Y_{[1,t]}$, being in state $i$ at time $t$. $\alpha_i\big(t\big)$ can be computed recursively using

\[\alpha_j\big(t+1\big) = b_j\big(y_{t+1}\big)\sum_{i=1}^N\alpha_i\big(t\big)a_{ij}\]

where $\alpha_i\big(1\big) = \pi_ib_j\big(y_1\big)$

\subsection*{Backward Pass}
Let $\beta_i\big(t\big) = P\big(Y_{[t+1,T]}|X_t=i,\theta\big)$, the probability of observing  $Y_{[t+1,T]}$, being in state $i$ at time $t$. $\beta_i\big(t\big)$ can be computed recursively using

\[\beta_i\big(t\big) = \sum_{j=1}^N\beta_j\big(t+1\big)a_{ij}b_j\big(y_{t+1}\big) \]

where $\beta_i\big(T\big) = 1$

\subsection*{Update}
Let $\gamma_i\big(t\big)$ be the probability of being in state $i$ at time $t$, having observed the sequence $Y_{[1,T]}$ and model parameters $\theta$.

\[\gamma_i\big(t\big) = P\big( X_t=i|Y_{[1,T]},\theta\big)\]

\[\gamma_i\big(t\big) = \frac{\alpha_i\big(t\big) \beta_i\big(t\big)}{\sum_{j=1}^N \alpha_i\big(t\big) \beta_i\big(t\big) }\]

Let $\xi_{ij}\big(t\big)$ be the probability of being in state $i$ and $j$ at time $t$ and $t+1$, having observed the sequence $Y_{[1,T]}$ and model parameters $\theta$.

\[\xi_{ij}\big(t\big) = P\big( X_t=i,X_{t+1}=j|Y_{[1,T]},\theta\big)\]

\[\xi_{ij}\big(t\big) = \frac{\alpha_i\big(t\big)a_{ij}\beta_j\big(t+1\big)b_j\big(y_{t+1}\big)}{\sum_{k=1}^K \alpha_k\big(T\big)}\]

Now, we can update the model parameter $\theta$ using $\gamma_i\big(t\big)$ and $\xi_{ij}\big(t\big)$.
\begin{itemize}
\item \( \pi_i^* = \gamma_i\big(1\big) \)	
\item \( a_{ij}^* = \frac{\sum_{t=1}^{T-1}\xi_{ij}\big(t\big)}{\sum_{t=1}^{T-1}\gamma_{i}\big(t\big)} \)
\item \( b_i^*\big( v_k \big) = \frac{\sum_{t=1}^{T}1_{y_t=v_k}\gamma_{i}\big(t\big)}{\sum_{t=1}^{T}\gamma_{i}\big(t\big)}\)
;  where $1_{y_t=v_k}$ is $1$ if $y_t=v_k$ else $0$.
\end{itemize}

\section{Spectral Learning}
\label{Spectral Learning}

In the above section we have discussed the EM algorithm for learning the parameters (transition matrix, emission densities and initial distribution ) of a Hidden Markov Model. One major drawback of the above algorithm is that it can possibly converge to a local optima. Since learning an HMM can be hard (Terwijin 2002 ) in some scenarios, several heuristics based approaches have been suggested for the learning process. However, in practical applications, the hardness scenarios are typically less encountered and under certain assumption a more accurate spectral learning algorithm has been proposed by Daniel Hsu , et. al.  This algorithm enables us to efficiently perform the following tasks.
\begin{enumerate}
	\item Approximate the joint liklihood of an observed sequence of length \textit{t}. The quality of approximation degrades as \textit{t} increases.
	\item Given a future observation, approximate its conditional distribution given some previous observation. 
\end{enumerate}
In the analysis presented in this report we are performing both of the above task to approximate the distribution of a future symbol.
\subsection{Priliminaries}
In this section we will we will briefly describe how an HMM can be represented in a matrix form, followed by the basic notation used in explaining the spectral algorithm. 
\newline
We will first consider the matrix notation of forward algorithm (as described in previous sections) an HMM. For each symbol /x, define a matrix $A_x$ as follows -
\begin{center}
$[A_x]_{h',h}$ = \textit{t}(h'$\vert$h)o(x$\vert$h) 

\end{center}
 
where t(h'$\vert$h) is the transition matrix and o is the emission density matrix.
Now consider a sequence of observed symbols as, $o_1,o_2...o_T$. The joint liklihood of the distribution given by $p(o_1....o_T)$ can be represented as -
\begin{center}
$p(o_1....o_T) = 1^T \times A_{o_T} \times A_{o_{T-1}} . . . . \times A_{o_1} \times \pi$
\end{center}
where $\pi$ is the initial state distribution. We will see that the spectral learning algorithm described in the later sections approximates $A_x$ and there by computes the joint distribution.

\subsection{Notation}
Before we describe the spectral learning algorithm, consider the following notations.
\begin{enumerate}
\item For each symbol \textit{x} define, 
\begin{center}
$[P_{3,x,1}]_{i,j} = P(X_3 = i, X_2 = x, X_1 = j)$
\end{center}
where $[P_{3,x,1}]_{i,j}$ denotes the joint probability distribution of the neighboring symbols for a current symbol.
\item For every symbol \textit{x} define,
\begin{center}
$[P_{2,1}]_{i,j} = P(X_2 = j, X_1 = i)$
\end{center}
where $[P_{2,1}]_{i,j}$ denotes the joint probability distribution for consecutive symbols in the observation sequence.
\end{enumerate}
\subsection{Spectral learning Algorithm}
Given the list of sequences of observed symbols, following steps outline the spectral learning algorithm.
\newline
\newline
\textbf{Step 1.} Estimate the empirical approximations of $[P_{3,x,1}]_{i,j}$ and $[P_{2,1}]_{i,j}$ as follows - 
\begin{enumerate}
 \item $[\hat{P}_{2,1}]_{i,j} = \frac{Count(X_2 = i, X_1 = j)}{N_{2,1}}$ ,where $N_{2,1}$ is the number total consecutive pairs. and,
\item $[\hat{P}_{3,x,1}]_{i,j} = \frac{Count(X_3 = i, X_2 = x, X_1 = j)}{N_{3,x,1}}$ ,where $N_{3,x,1}$ is the total number of triples for a particular symbol x.
\end{enumerate}

\textbf{Step 2.} Compute singular value decomposition of $[\hat{P}_{2,1}]_{i,j}$ 
\begin{center} SVD($[\hat{P}_{2,1}]_{i,j}$) = $U \in R^{n\times m} ,  \Sigma \in R^{m\times m} , V \in R^{n \times m}$
\end{center}
where \textit{m} denotes the rank and is analogous to the number of hidden states in our algorithm.

\textbf{Step 3.} For each symbol x compute $B_x$ as follows -
\begin{center}
$B_x = U^T \times [P_{3,x,1}] \times V \times \Sigma^{-1}$
\end{center}

At this point, it is important that we state the following theorem. We skip the proof for brevity.

\textbf{Theorem 1.} If $P_{2,1}$ is of rank m then,
\begin{center}
$B_x = G \times A_X \times G^{-1}$
\end{center}
where $G \in R^{m \times m}$ is an invertible matrix.

\textbf{Step 4.} Compute $B_{0}$ and $B_{\inf}$
\begin{enumerate}
\item $b_{0} = G\pi$ 
\item $b_{\inf} = 1^TG^{-1}$
\end{enumerate}

\textbf{Step 5.} For a given sequence $o_1$, $o_2$, ... $o_T$ and a future symbol $o_{T+1}$ we can compute the joint liklihood of the sequence and the conditional distribution of the future symbol as follows.
\begin{enumerate}
\item $\hat{p}(o_1, o_2, ... o_T) = b^{\inf} \times B_{o_{T}} . . . B_{o_{1}} \times b^{0}$, 
\newline where $\hat{p}(o_1, o_2, ... o_T)$ represents the joint liklihood. Note that all the G's cancel out and we arrive at the forward matrix form of an hmm.

\item $\hat{p}(o_{T+1} \Vert o_{1}...o_{T} ) = \frac{\hat{p}(o_1, o_2, ... o_T, o_{T+1})}{\hat{p}(o_1, o_2, ... o_T)} \propto \hat{p}(o_1, o_2, ... o_T, o_{T+1})$ 

\end{enumerate}

\section{Dataset}
The dataset we are using for this report is taken from the SPICE challenge. SPICE challenge is a competition where the task is to predict the next element given history. But there is caveat, the data set does not give away how and from where the data is collected like natural language data or DNA sequence data etc. The is basically anonymous with respect the source of generation. Some of the data can also be synthetic but it is not known prior which data set is real world data or which data set is synthetic. The link to the training data set is public and is quoted in the references section. Testing is done online, for each data set that you train, we have to send predictions to the sequences sent by the server online. The accuracy is measured using the NDCG score that we shall describe in the next section.

\section{Normalized discounted cumulative gain (NDCG)}
Normalized discounted cumulative gain (NDCG) measures the performance of a recommendation system based on the graded relevance of the recommended entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal ranking of the entities. This metric is frequently used in information retrieval and to evaluate the performance of web search engines. In our case we have to predict best possible next five elements. If there was an ideal five next elements for the next five elements which we want compare with, NDCG score gives a natural way of this very issue similar to the ranking of search results.

Here is the NDCG score as described by the competition:\\
Suppose the test set is made of prefixes $y_1, ..., y_M$. Suppose now that the distinct next symbols ranking submitted for $y_i$ is $(\hat{a}_1^i, ..., \hat{a}_5^i)$ sorted from more likely to less likely. The competition organizers have access to $p(.|y_i)$, the target probability distribution of possible next symbols knowing the prefix $y_i$, therefore they can compute the following measure for prefix $y_i$:

$NDCG_5\big(\hat{a}_1^i, ..., \hat{a}_5^i\big) = \frac{\sum\limits_{k=1}^5 \frac{p(\hat{a}_i^k|y_i)}{log_2(k+1)}}{\sum\limits_{k=1}^5 \frac{p_k}{log_2(k+1)}}$

where $p1 >= p2 >= ... >= p5$ are the top 5 values in the distribution $p(.|yi)$.

This NDCG score is returned back when a submission is made for a model built using a particular data set.

\section{Results}
We have built models using three different techniques as described above. They are n-gram models, HMM using EM method and spectral learning. We have tuned our models using cross validation on the training data set and then tested the models using the submission code given by the competition. The testing works as follows. Firstly, it is online and therefore each time the submission code gets a sequence and triggers our models to give top 5 predictions. It then reports the top 5 predictions to the server. The server then gives the next sequence and the process continues till all the test sequences are done. At the end of testing, it returns the NDCG score of our predictions. We are reporting these scores in the following tables for all the three methods we have investigated.
\subsection{Tabular Results}
\begin{table}[H]
\caption{N-Gram Model}
\label{N-Gram_model}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf \# Symbols} &\multicolumn{1}{|c|}{\bf 3-gram backoff} &\multicolumn{1}{|c|}{\bf 5-grambackoff}\\
\hline
\multirow{1}{*}{Data Set 1}& \multirow{1}{*}{20} & 0.46 & 0.28 \\
\hline
\multirow{1}{*}{Data Set 2}& \multirow{1}{*}{10} & 0.38 & 0.46  \\
\hline
\multirow{1}{*}{Data Set 3}& \multirow{1}{*}{10} & 0.34 & 0.42 \\
\hline
\multirow{1}{*}{Data Set 4}& \multirow{1}{*}{33} & 0.53 & 0.35 \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[H]
\caption{Hidden Markov Model using EM}
\label{HMM_EM}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf\# Symbols} &\multicolumn{1}{|c|}{\bf n-states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{20} & 5 & 0.84 \\
& & 10 & 0.84 \\
\hline
\multirow{2}{*}{Data Set 2}& \multirow{2}{*}{10} & 3 & 0.77 \\
& & 5 & 0.76 \\
\hline
\multirow{2}{*}{Data Set 3}& \multirow{2}{*}{10} & 3 & 0.72 \\
& & 5 & 0.70 \\
\hline
\multirow{2}{*}{Data Set 4}& \multirow{2}{*}{33} & 7 & 0.26 \\
& & 15 & 0.27 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[H]
\caption{Hidden Markov Model using Spectral Learning}
\label{Spectral Learning}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf \# Symbols} &\multicolumn{1}{|c|}{\bf \# hidden states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{20} & 5 & 0.64 \\
& & 10 & 0.65 \\
\hline
\multirow{2}{*}{Data Set 2}& \multirow{2}{*}{10} & 3 & 0.602 \\
& & 5 & 0.60 \\
\hline
\multirow{2}{*}{Data Set 3}& \multirow{2}{*}{10} & 3 & 0.77 \\
& & 5 & 0.82 \\
\hline
\multirow{2}{*}{Data Set 4}& \multirow{2}{*}{33} & 7 & 0.33 \\
& & 10 & 0.34 \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{References}
\small{
[1] Daniel Hsu \& Sham M. Kakade \& Tong Zhang, {\it A Spectral Algorithm for Learning Hidden Markov Models}

[2] Frazzoli, Emilio. {\it Intro to Hidden Markov Models the Baum-Welch Algorithm.}

[3] Rabiner, Lawrence First {\it Hand: The Hidden Markov Model}  IEEE Global History Network. Retrieved 2 October 2013

[4] {Manning:2008:IIR:1394399,
 author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
 title = {Introduction to Information Retrieval},
 year = {2008},
 isbn = {0521865719, 9780521865715},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 

[5] Zhong Su, Qiang Yang, Ye Lu and Hongjiang Zhang, "WhatNext: a prediction system for Web requests using n-gram sequence models," Web Information Systems Engineering, 2000. Proceedings of the First International Conference on, Hong Kong, 2000, pp. 214-221 vol.1.
doi: 10.1109/WISE.2000.882395

[6] N-gram. (2016, May 9). In Wikipedia, The Free Encyclopedia. Retrieved 02:28, June 11, 2016, from https://en.wikipedia.org/w/index.php?title=N-gram\&oldid=719361080

[7] DataSet from SPICE Challenge, website: http://spice.lif.univ-mrs.fr/


\end{document}
