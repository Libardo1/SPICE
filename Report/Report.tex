\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Sequence Prediction using graphical Models }

\author{
Akshay Jain\\
Department of Computer Science\\
University of California,Irvine \\
\texttt{akshaj1@uci.edu} \\
\And
Varun Bharill \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
\And
Sai Teja \\
Department of Computer Science\\
University of California,Irvine \\
\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Being able to guess the next element of a sequence is an important question in many fields, from natural language processing (NLP) to biology (DNA modelisation for instance). Different algorithms have been suggested in literature to handle the problem. In this paper, we explore Graphical model based algorithms and how they perform with respect to each other.
\end{abstract}

\section{Introduction}
\label{Introduction}
\label{Introduction}
Sequence prediction is an important task in machine learning. Sequence prediction can be defined as the task of predicting the next element given the previous element in a sequence. This task appears naturally in stock market prediction, user browsing action prediction, word prediction in natural languages. Sequence prediction problem also arises prominently in time series data rain fall measurement etc. 

There are various machine learning techniques developed or existing techniques applied to effectively predict the next element in a sequence. The algorithms for sequence prediction can be broadly classified into graphical models, models based on expert advice and recurrent neural network models. Recurrent neural networks and graphical models have been investigated extensively. 

The study of sequence prediction has always been specific to a particular field like Natural 

\section{Algorithms}
\label{Algorithms}
Other the period, various algorithms have been developed to predict the future values in the sequence data. 
\subsection{N-Gram Model}

\subsection{Hidden Markov Model(HMMs)}

\subsection{Other}

In this paper, we will focus mainly on N-gram models and Hidden Markov Models.

\section{N-gram Model}
\label{N-Gram Model}

\section{Hidden Markov Model}
\label{HMM}

In simple Markov Model,the states are visible. In Hidden Markov Model, the states are not visible, but the output dependent on the states are observed. Each states has a probability distribution over the possible observations. The sequence of the observations allows to infer the sequence of the hidden states. 

Consider \(x_t\) be the observed data variable, where t denotes the discrete time or position stamp in the sequence 1...T

The complete observed sequence consists of \(D ={x_1,,,,x_T}\)

\subsection{Learning Model Parameters}
\subsubsection{EM algorithm}
\subsubsection{Spectral Learning}

\section{EM algorithm}
\label{EM algorithm}
\section{Spectral Learning}
\label{Spectral Learning}

\section{Dataset}

\section{Reults}

\begin{table}[t]
\caption{N-Gram Model}
\label{N-Gram_model}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-gram} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{Hidden Markov Model using EM}
\label{HMM_EM}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-states} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Hidden Markov Model using Spectral Learning}
\label{Spectral Learning}
\begin{center}
\begin{tabular}{ |l|l|l|l| }
\hline
\multicolumn{1}{|c|}{\bf Data Set} &\multicolumn{1}{|c|}{\bf No Of Observations} &\multicolumn{1}{|c|}{\bf n-gram} &\multicolumn{1}{|c|}{\bf Accuracy}\\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\multirow{2}{*}{Data Set 1}& \multirow{2}{*}{1} & 3 & - \\
& & 4 & - \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
